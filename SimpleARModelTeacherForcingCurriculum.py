# -*- coding: utf-8 -*-
"""CaboFrio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f8HKgr8mVEmL2SbcbFqJiXyAnPw4QYjN
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import datetime
import matplotlib.pyplot as plt
import math
import random
from typing import List, Tuple
import pathlib
import os
import pandas as pd
from scipy.stats import mannwhitneyu
import polars as pl


p = pathlib.Path("/home/lucapatitucci/envio/")
#flag = "estats"  # pode ser estats ou janelas


"""#Estraindo os dados"""

class SantosDataset:
    def __init__(
        self,
        data_path: pathlib.Path,
    ):
        if not data_path.exists():
            raise FileNotFoundError(f"Path {data_path} does not exist.")

        if not data_path.is_dir():
            raise NotADirectoryError(f"Path {data_path} is not a directory.")

        self.data_path = data_path
        
        self.original_data = {
            f.stem: (df := pl.read_parquet(f))
            .with_columns(
                [
                    (pl.col("datetime") - pl.col("datetime").min())
                    .dt.total_minutes()
                    .cast(pl.Float32)
                    .alias("rel_datetime")
                ]
            )
            .select(
                ["datetime", "rel_datetime"]
                + [col for col in df.columns if col not in ["rel_datetime", "datetime"]]
            )
            for f in self.data_path.glob("*.parquet")
            if f.is_file()
        }

        self.min_timestamp: float = min(
            df["rel_datetime"].min() for df in self.original_data.values()
        )
        self.max_timestamp: float = max(
            df["rel_datetime"].max() for df in self.original_data.values()
        )




class PositionalEncoding(nn.Module):
    """Encoder that applies positional based encoding.

    Encoder that considers data temporal position in the time series' tensor to provide
    a encoding based on harmonic functions.

    Attributes:
        hidden_size (int): size of hidden representation
        dropout (nn.Module): dropout layer
        div_term (torch.Tensor): tensor with exponential based values, used to encode timestamps


    """

    def __init__(self, time_encoding_size: int, dropout: float, **kwargs):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        self.hidden_size = time_encoding_size
        self.div_term = torch.exp(
            torch.arange(0, self.hidden_size, 2).float()
            * (-math.log(10000.0) / self.hidden_size)
        )

    def forward(self, position):
        """Encoder's forward procedure.

        Encodes time information based on temporal position. In the encoding's
        vector, for even positions employs sin function over position*div_term and for
        odds positions uses cos function. Then, applies dropout layer to resulting tensor.

        Args:
            torch.tensor[int]: temporal positions for encoding

        Returns:
            torch.tensor[float]: temporal encoding
        """

        pe = torch.empty(position.shape[0], self.hidden_size, device=position.device)
        pe[:, 0::2] = torch.sin(position * self.div_term.to(position.device))
        pe[:, 1::2] = torch.cos(position * self.div_term.to(position.device))
        return self.dropout(pe)


"""##Criando os dataloaders de treino e teste"""


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


batch_size = 32
num_epochs = 500
input_size = 1

hiperparams_base = {
    "past_len": datetime.timedelta(days=8),
    "future_len": datetime.timedelta(days=2),
    "sequence_step": datetime.timedelta(hours=12),
    "min_data_points_context": 50,
    "min_data_points_forecast": 10,
    "hidden_size": 16,
    "learning_rate": 1e-3,
}
hiperparams_lists = {
    "past_len": [
        datetime.timedelta(days=2),
        datetime.timedelta(days=4),
        datetime.timedelta(days=6),
        datetime.timedelta(days=8),
        datetime.timedelta(days=12),
        datetime.timedelta(days=16),
    ],
    "future_len": [
        datetime.timedelta(days=1),
        datetime.timedelta(days=1, hours=8),
        datetime.timedelta(days=2),
        datetime.timedelta(days=3),
        datetime.timedelta(days=4),
    ],
    "sequence_step": [
        datetime.timedelta(hours=6),
        datetime.timedelta(hours=8),
        datetime.timedelta(hours=12),
        datetime.timedelta(hours=18),
        datetime.timedelta(hours=24),
    ],
    "min_data_points_context": [25, 35, 50, 75, 100],
    "min_data_points_forecast": [5, 7, 10, 15, 20],
    "hidden_size": [8, 16, 32, 43, 64, 96, 128],
    "learning_rate": [1e-5, 1e-4],
}
time_encoder_config = {
    "time_encoding_size": 30,
    "dropout": 0,
}
rnn_config = {
    "input_size": input_size + time_encoder_config["time_encoding_size"],
    "hidden_size": hiperparams_base["hidden_size"],
    "learning_rate": hiperparams_base["learning_rate"],
}

class TimeSeries:
    def __init__(
        self,
        x: list[torch.Tensor],
        x_timestamps: list[torch.Tensor],
        x_len: list[int],
        y: list[torch.Tensor],
        y_timestamps: list[torch.Tensor],
        inference_times: list[torch.Tensor],
    ):
        self.x = x
        self.x_timestamps = x_timestamps
        self.x_len = x_len
        self.y = list(y)
        self.y_timestamps = y_timestamps
        self.inference_times = inference_times

    def change_device(self, device):
        self.x = [xi.to(device) for xi in self.x]
        self.y = [yi.to(device) for yi in self.y]


from torch.nn.utils.rnn import pad_sequence


def sample_random_t_inference(
    min_timestamp: float,
    max_timestamp: float,
    max_context_window_size: float,
    max_forecast_window_size: float | None = None,
) -> torch.Tensor:
    base_random = torch.rand(1)

    lbound = min_timestamp + max_context_window_size
    ubound = (
        max_timestamp
        if max_forecast_window_size is None
        else max_timestamp - max_forecast_window_size
    )
    interval = (ubound - lbound) * base_random
    t_inference = lbound + interval

    return t_inference


class VelocidadeCorrentesDataset(Dataset):

    def __init__(
        self,
        features: np.ndarray,
        timestamps: list[datetime.datetime],
        min_data_points_context: int,
        min_data_points_forecast: int,
        past_len: datetime.timedelta,
        future_len: datetime.timedelta,
        dataset_length: int | None = None,
        sequence_step: datetime.timedelta | None = None,
        mean: np.ndarray | None = None,
        std: np.ndarray | None = None,
    ):

        if dataset_length is None and sequence_step is None:
            raise ValueError("dataset_length and sequence_step cannot be both None")

        if mean is None or std is None:
            self.mean = np.mean(features, axis=0)
            self.std = np.std(features, axis=0)
        else:
            self.mean = mean
            self.std = std

        self.min_data_points_context = min_data_points_context
        self.min_data_points_forecast = min_data_points_forecast

        self.min_datetime, self.max_datetime = timestamps[0], timestamps[-1]
        self.features = torch.tensor(
            (train_features - self.mean) / self.std, dtype=torch.float32
        )

        self.original_timestamps = timestamps

        self.timestamps = torch.tensor(
            [(t - self.original_timestamps[0]).total_seconds() for t in timestamps],
            dtype=torch.float32,
        )

        self.past_len = past_len.total_seconds()
        self.future_len = future_len.total_seconds()

        self.time_series_len_in_seconds = self.timestamps[-1] - self.timestamps[0]

        self.t_inferences = None
        self.dataset_length = dataset_length
        if sequence_step is not None:
            self.t_inferences = torch.arange(
                self.timestamps[0] + self.past_len,
                self.timestamps[-1] - self.future_len,
                sequence_step.total_seconds(),
            )

    def desnormaliza(self, input: np.ndarray):
        return input * self.std + self.mean

    def __getitem__(self, index: int):

        t_inference = (
            sample_random_t_inference(
                min_timestamp=0,
                max_timestamp=self.time_series_len_in_seconds,
                max_context_window_size=self.past_len,
                max_forecast_window_size=self.future_len,
            )
            if self.t_inferences is None
            else self.t_inferences[index]
        )

        lbound_index = torch.searchsorted(self.timestamps, t_inference - self.past_len)
        t_inf_index = torch.searchsorted(self.timestamps, t_inference)
        ubound_index = torch.searchsorted(
            self.timestamps, t_inference + self.future_len
        )

        context = self.features[lbound_index:t_inf_index]
        context_timestamps = self.timestamps[lbound_index:t_inf_index]
        x_len = t_inf_index - lbound_index + 1
        if context.shape[0] < self.min_data_points_context:
            return self.__getitem__(index)

        forecast = self.features[t_inf_index:ubound_index]
        forecast_timestamps = self.timestamps[t_inf_index:ubound_index]

        if forecast.shape[0] < self.min_data_points_forecast:
            return self.__getitem__(index)

        return (
            context,
            context_timestamps,
            x_len,
            forecast,
            forecast_timestamps,
            t_inference,
        )

    def __len__(self):
        return (
            self.t_inferences.shape[0]
            if self.t_inferences is not None
            else self.dataset_length
        )


def pad_fn(data):
    contexts, context_timestamps, x_len, forecasts, forecast_timestamps, inference_times = zip(
        *data
    )

    timeSeries = TimeSeries(
        x=contexts,
        x_timestamps=context_timestamps,
        x_len=x_len,
        y=forecasts,
        y_timestamps=forecast_timestamps,
        inference_times=inference_times,
    )

    return timeSeries


#features = np.stack((u_sem_rep, v_sem_rep), axis=-1)
original_data = SantosDataset(p).original_data
tempo_em_data = original_data["datetime"]
u = original_data["u"].to_numpy()

features = np.array(u, dtype= np.float64)
train_split = int(0.9    * len(tempo_em_data[:]))
train_features = features[:train_split]
train_timestamps = tempo_em_data[:train_split]

test_features = features[train_split:]
test_timestamps = tempo_em_data[train_split:]

def generate_dataloaders(min_data_points_context: int, min_data_points_forecast: int, sequence_step: datetime.timedelta, past_len: datetime.timedelta, future_len: datetime.timedelta) -> tuple[DataLoader, DataLoader]:

    train_dataset = VelocidadeCorrentesDataset(
        features=train_features,
        timestamps=train_timestamps,
        min_data_points_context= min_data_points_context,
        min_data_points_forecast=min_data_points_forecast,
        past_len=past_len,
        future_len=future_len,
        dataset_length=1000,
    )


    test_dataset = VelocidadeCorrentesDataset(
        features=test_features,
        timestamps=test_timestamps,
        min_data_points_context=min_data_points_context,
        min_data_points_forecast=min_data_points_forecast,
        past_len=past_len,
        future_len=future_len,
        sequence_step=sequence_step,
    )

    train_dataloader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=pad_fn,
        drop_last=True,
    )


    test_dataloader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=pad_fn,
        drop_last=True,
    )
    return (train_dataloader, test_dataloader)


"""# Modelo - treino"""


class SimpleARModelTeacherForcingCurriculum(nn.Module):
    def __init__(self, input_size, hidden_size, time_encoder_config, **kwargs):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.time_encoder = PositionalEncoding(**time_encoder_config)
        self.rnn = nn.GRU(input_size, hidden_size, batch_first=True)
        self.linear = nn.Linear(input_size, hidden_size)

    def encode_gaps(time_encoder, x_gaps, y_gaps):
        time_encoder.eval()
        with torch.no_grad():
            x_encoded = [
                np.array(time_encoder(torch.unsqueeze(gaps, -1)).cpu())
                for gaps in x_gaps
            ]
            y_encoded = [
                np.array(time_encoder(torch.unsqueeze(gaps, -1)).cpu())
                for gaps in y_gaps
            ]
        return x_encoded, y_encoded

    def encode(
        self,
        context: list[torch.Tensor],
        context_timestamps: list[torch.Tensor],
        context_lenghts: list[int]
    ):

        x = pad_sequence(context, batch_first=True)

        timestamps = pad_sequence(context_timestamps, batch_first=True)

        features = torch.cat((x, timestamps), dim=-1)

        out, _ = self.rnn(features)
        last_out = torch.stack(
            [out[i, xL - 1] for i, xL in enumerate(context_lenghts)], dim=0
        )
        return last_out

    def decode(
        self,
        time_series: TimeSeries,
        last_out_encoder: torch.Tensor,
        encoded_y_timestamps: List[torch.Tensor]
    ):
        inp = self.linear(last_out_encoder)

        h_n = last_out_encoder.unsqueeze(0)
        output_seq = torch.empty(
            time_series.x.shape[0],
            max(time_series.y_len),
            time_series.x.shape[-1],
            device=inp.device,
        )  # allocate memory for the output sequence
        output_seq[:, 0, :] = inp

        max_y_len = max([y.shape[0] for y in time_series.y])

        for i in range(1, max_y_len):
            rnn_inp = torch.cat((inp, encoded_y_timestamps[i - 1]), dim=-1)
            out, h_n = self.rnn(rnn_inp, h_n)
            inp = self.linear(out.squeeze(1))
            output_seq[:, i] = self.linear(out.squeeze(1))
    
        results = []
        for i, yL in enumerate(time_series.y_len):
            results.append(output_seq[i, :yL])
        return results

    def forward(
        self,
        time_series: TimeSeries
    ):
        
        encoded_x_timestamps = []
        for i, ts in enumerate(time_series.x_timestamps):
            out = ts.roll(-1)
            out[-1] = time_series.y_timestamps[i][0]
            out = out - time_series.inference_times[i]
            out = self.time_encoder(out.unsqueeze(-1))
            encoded_x_timestamps.append(out)

        encoded_y_timestamps = []
        for i, ts in enumerate(time_series.y_timestamps):
            out = ts.roll(-1)
            out = out[:-1]
            out = out - time_series.inference_times[i]
            out = self.time_encoder(out)
            encoded_y_timestamps.append(out)

        last_out = self.encode(time_series.x, encoded_x_timestamps, time_series.x_len)
        results = self.decode(time_series, last_out, encoded_y_timestamps)
        return results


def index_agreement_torch(s: torch.Tensor, o: torch.Tensor) -> torch.Tensor:
    """
    index of agreement
    Willmott (1981, 1982)

    Args:
        s: simulated
        o: observed

    Returns:
        ia: index of agreement
    """
    o_bar = torch.mean(o, dim=0)
    ia = 1 - (torch.sum((o - s) ** 2, dim=0)) / (
        torch.sum(
            (torch.abs(s - o_bar) + torch.abs(o - o_bar)) ** 2,
            dim=0,
        )
    )

    return ia.mean()


indexOfAgreementLoss = lambda: lambda s, o: -index_agreement_torch(s, o) + 1.0


def atualiza_rnn_config(rnn_config: dict, hidden_size: int):
    rnn_config["hidden_size"] = hidden_size


def inicializa_modelo(rnn_config, time_encoder_config):
    model = SimpleARModelTeacherForcingCurriculum(
        **rnn_config, time_encoder_config=time_encoder_config
    ).to(device)
    optimizer = optim.Adam(model.parameters(), lr=rnn_config["learning_rate"])
    return model, optimizer


def train_loop(
    model,
    optimizer,
    criterion,
    train_dataloader: DataLoader,
    test_dataloader: DataLoader,
):
    train_loss_values = []
    test_loss_values = []
    df_window_plt = []
    for epoch in range(num_epochs):
        model.train()
        epoch_train_loss = 0
        for timeSeries in tqdm(train_dataloader):
            timeSeries.change_device(device)
            outputs = model(timeSeries)
            loss = 0
            for i in range(len(outputs)):
                loss += criterion(outputs[i], timeSeries.y[i])

            loss /= len(outputs)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            epoch_train_loss += loss.detach().cpu()
        model.eval()
        with torch.no_grad():
            epoch_test_loss = 0
            batch_show = int(
                random.random() * len(test_dataloader)
            )  # escolha aleatoria de um batch de cada epoca para extrair resultados para visualizar o desempenho
            for i, (timeSeries) in enumerate(tqdm(test_dataloader)):
                timeSeries.change_device(device)
                outputs = model(timeSeries)
                test_loss = 0
                for j in range(len(outputs)):
                    test_loss += criterion(outputs[j], timeSeries.y[j])
                test_loss /= len(outputs)

                epoch_test_loss += test_loss.detach().cpu()

                if i == batch_show:
                    plt_item = {
                        "contexts_plt": timeSeries.x[0][: timeSeries.x_len[0]].cpu(),
                        "forecasts_plt": timeSeries.y[0].cpu(),
                        "outputs_plt": outputs[0].cpu(),
                        "timestamp_plt": timeSeries.timeStamps[0],
                        "epoch": epoch,
                    }
                    df_window_plt.append(plt_item)
        train_loss_values.append(epoch_train_loss / len(train_dataloader))
        test_loss_values.append(epoch_test_loss / len(test_dataloader))

    return {
        "train_loss_values": train_loss_values,
        "test_loss_values": test_loss_values,
    }, df_window_plt


def treinamento(rnn_config, time_encoder_config):
    model, optimizer = inicializa_modelo(rnn_config, time_encoder_config)
    train_dataloader, test_dataloader = generate_dataloaders(hiperparams_base["min_data_points_context"], hiperparams_base["min_data_points_forecast"], 
                                                             hiperparams_base["sequence_step"], hiperparams_base["past_len"], hiperparams_base["future_len"])
    
    losses, winds = train_loop(model, optimizer, indexOfAgreementLoss(), train_dataloader, test_dataloader)
    torch.save(model, '/home/lucapatitucci/envio/model_parameters.pt')

treinamento(rnn_config, time_encoder_config)


"""#Resultados - utilizar matplotlib para plotar:
* loss treinamento
* loss validação
* Escolher alguns dados do set de validação e comparar a série esperada vs a série prevista (saindo da janela de cotexto usar uma cor para a seq prevista e outra pra seq esperada)
* Testar a rede para hiperparâmetros diferentes (ex ver como o loss aumenta com maiores janelas de forecast)
"""




def save_figure(fig: plt.figure, sample_file_name: str):
    plt.figure(num=fig)
    script_dir = os.path.dirname("/home/lucapatitucci/envio/cabofrio.py")
    results_dir = os.path.join(script_dir, "Results/")

    if not os.path.isdir(results_dir):
        os.makedirs(results_dir)

    plt.savefig(results_dir + sample_file_name)


def process_default(model, timeSeries: TimeSeries) -> tuple[List, List]:
    simulated = model(timeSeries)
    observed = timeSeries.y
    return (simulated, observed)


def process_future_len(model, timeSeries: TimeSeries) -> tuple[List, List]:
    indices_t_inferece = np.searchsorted(tempo_em_data, timeSeries.timeStamps)
    indices_fim = np.searchsorted(
        tempo_em_data,
        np.array(timeSeries.timeStamps) + min(hiperparams_lists["future_len"]),
    )
    janelas_avaliacao = [
        torch.tensor(features[indices_t_inferece[i] : indices_fim[i]], device=device)
        for i in range(len(indices_t_inferece))
    ]
    timeSeries = TimeSeries(
        timeSeries.x,
        [len(x) for x in timeSeries.x],
        janelas_avaliacao,
        [len(y) for y in janelas_avaliacao],
        timeSeries.timeStamps,
    )
    simulated = model(timeSeries)
    observed = janelas_avaliacao
    return (simulated, observed)


hiperparams_funcs = {
    "past_len": process_default,
    "future_len": process_future_len,
    "sequence_step": process_default,
    "min_data_points_context": process_default,
    "min_data_points_forecast": process_default,
    "hidden_size": process_default,
    "learning_rate": process_default,
}


def teste_hiperparametros(hiperparams: dict, hiperparametro_testado: str):
    hiperparam_values = hiperparams_lists[hiperparametro_testado]
    df_losses = []
    df_IoA = []

    for hiperparam_value in hiperparam_values:
        hiperparams[hiperparametro_testado] = hiperparam_value
        train_dataloader, test_dataloader = generate_dataloaders(
            hiperparams["min_data_points_context"],
            hiperparams["min_data_points_forecast"],
            hiperparams["sequence_step"],
            hiperparams["past_len"],
            hiperparams["future_len"],
        )
        if train_dataloader is None or test_dataloader is None:
            df_losses.append(
                {
                    "train_loss_values": [],
                    "test_loss_values": [],
                    "hiperparam": hiperparam_value,
                }
            )
            continue
        atualiza_rnn_config(rnn_config, hiperparams["hidden_size"])
        model, optimizer = inicializa_modelo(rnn_config, time_encoder_config)
        losses_item, _ = train_loop(
            model, optimizer, indexOfAgreementLoss(), train_dataloader, test_dataloader
        )
        losses_item["hiperparam"] = hiperparam_value
        df_losses.append(losses_item)
        index_of_agreement_values = []
        model.eval()
        with torch.no_grad():
            for timeSeries in test_dataloader:  # Mudar forma de avaliar IoA aqui
                timeSeries.change_device(device)
                simulated, observed = hiperparams_funcs[hiperparametro_testado](
                    model, timeSeries
                )
                for i in range(len(simulated)):
                    index_of_agreement_values.append(
                        index_agreement_torch(simulated[i], observed[i]).cpu()
                    )
        df_IoA.append((index_of_agreement_values, hiperparam_value))
    # df_IoA = pd.DataFrame(df_IoA, columns= ['IoA_values', 'Hyperparam_value'])
    return df_losses, df_IoA


# Loss curves A partir daqui o código sera movido para outro arquivo .py, funcoes de visualizacao 
# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
# ax1.set_title("Training loss curves")
# ax1.set_ylabel("Loss")
# ax1.set_xlabel("Epochs")
# ax2.set_title("Test loss curves")
# ax2.set_ylabel("Loss")
# ax2.set_xlabel("Epochs")
# epoch_count = np.arange(num_epochs)


# def plot_losses_curves(
#     train_loss_values: List[float], test_loss_values: List[float], label: str
# ):
#     ax1.plot(epoch_count, train_loss_values, label=label, alpha=0.7)
#     ax2.plot(epoch_count, test_loss_values, label=label, alpha=0.7)

#     ax1.legend()
#     ax2.legend()


# if flag == "estats":
#     hiperparam_testado = "learning_rate"
#     df_losses, df_IoA = teste_hiperparametros(hiperparams_base, hiperparam_testado)
#     for losses_item in df_losses:
#         if (
#             len(losses_item["train_loss_values"]) > 0
#             and len(losses_item["test_loss_values"]) > 0
#         ):
#             plot_losses_curves(
#                 losses_item["train_loss_values"],
#                 losses_item["test_loss_values"],
#                 hiperparam_testado + ": " + str(losses_item["hiperparam"]),
#             )  # Aqui se algum dataloader vazio for feito ele vai pegar o i como se nao tivesse acontcido

#     save_figure(fig, "losses_" + hiperparam_testado)

